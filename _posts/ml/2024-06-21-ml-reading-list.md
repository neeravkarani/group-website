---
layout: post
title: "Reading list"
---

### Classics
 - [Learning representations by back-propagating errors](https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf)

<br>

### Seq-to-seq modeling
 - [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
 - [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
 - [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

<br>

### Residual networks
 - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
 - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027)

<br>

### Courses
- [CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/)

<br>

### Misc
- [The First Law of Complexodynamics](https://scottaaronson.blog/?p=762)
- [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton](https://arxiv.org/abs/1405.6903)
- [A tutorial introduction to the minimum description length principle](https://arxiv.org/abs/math/0406077)
