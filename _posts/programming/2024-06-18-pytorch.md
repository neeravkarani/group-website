---
layout: default
title: "PyTorch Basics"
---

## Introduction
PyTorch leverages CUDA (a parallel computing platform developed by NVIDIA for general computing on GPUs) to enable you to run machine learning code on GPUs.

## Tensors
Tensors are numerical representations of data (text, images, videos, audio, etc.).
 
### [A] Creating tensors
We can create tensors directly by specifying the data in the tensors using the torch.tensor() function, or with random numbers in a certain shape using the torch.rand() function. Other commons ways to create tensors are using the functions torch.zeros(), torch.ones(), torch.zeros_like(), torch.ones_like() and torch.arange().
 
Apart from the data that is present in the tensors, three other parameters that are important to specify when creating a tensor are: dtype (what data type is the tensor e.g. torch.float16 or torch.float32), device (what device is your tensor on), requires_grad (whether or not to track gradients with the operations on this tensor). We can cast tensors into different data types using tensor_new = tensor_old.type(torch.datatype).
 
We can create a tensor from a numpy array using the torch.from_numpy(ndarray) function, and create a numpy array from a torch tensor using the torch.Tensor.numpy() function. Note that the default data type of PyTorch is torch.float32, and that of NumPy is float64.

### [B] Manipulating tensors
We can manipulate tensors by using the following operations: addition, subtraction, element-wise multiplication and element-wise division. These are best done using the python symbols $+$, $-$, $*$ and $/$. When applying these operations on tensors whose shapes do not match, broadcasting rules will be applied. We can also do a different type of multiplication: matrix multiplication. This can be done using the torch.matmul() function or using @ operator. torch.einsum() is an excellent function for generic product and sum operations on two tensors along different dimensions. Numerous examples are shown here, here and here.

### [C] Manipulating tensor shapes
We can reshape a tensor using the torch.reshape() or the torch.Tensor.view() function. We can concatenate a sequence of tensors along an existing dimension using the torch.cat() function and along a new dimension using the torch.stack() function. We can remove dimensions of size 1 using the torch.squeeze() function, and add dimensions with size 1 using the torch.unsqueeze() function. We can rearrange the dimensions of a tensor in a specified order using the torch.permute() function, or we can swap any two axes of a tensor using the torch.transpose() function. As explained here, some of these operations do not generate a new tensor with a new layout, but just modify meta information in the Tensor object. But contiguity of storage may be necessary for some downstream functions. We can determine contiguity by torch.Tensor.is_contiguous() and use torch.Tensor.contiguous() to make a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data.

### [D] Aggregating tensors
We can find the minimum, maximum and mean values in a tensor using the torch.min(), torch.max() and torch.mean() functions. We can sum all values in a tensor using the torch.sum() function. In all these functions, we can also specify the dim along which to apply these operations, and also whether or not we would want the output to preserve the original dimensionality. We can get the indices of the min and max values using the torch.argmin() and the torch.argmax() functions.

<br>

## Neural Networks

### [A] Creating models
Torch.nn contains all of PyTorch's building blocks for neural networks. Almost all neural network models that we will build in PyTorch will inherit from the nn.Module class. This class contains the basic building blocks that all neural networks will use.

Class constructor: In the init() function of our model class, we will define learnable parameters either directly using the nn.Parameter class (Parameters are torch.Tensor subclasses that, when used within nn.Module, are automatically added to the list of its parameters, and will appear e.g. in model.parameters() iterator) or using predefined layers such as nn.Linear, nn.Conv2d, non-linear activation layers, etc. torch.nn.Sequential and torch.nn.ModuleList are two ways of chaining together multiple layers.

Forward method: The model class should also contain a forward() method that defines the computation to be performed at every call. This function takes data as inputs, computes the forward pass of the neural network using the input data and the model parameters and returns the model outputs.

Once the model is created, model.parameters() returns an iterator over all the parameters of the model, and model.state_dict() returns a dictionary of all parameters of the model and their current values. Once created, we can move our model to a GPU using the torch.nn.Module.to() function.

We can use torchinfo to print summary of the model's architecture.

### [B] Training models
Once built, we now have to train our models. Here, we usually follow the following steps in order. (1) We set our model in training mode via model.train(). This needs to be done because certain layers e.g. batch normalization behave differently at training and test time. (2) We define appropriate loss functions that measure discrepancy between the current and desired outputs of our models. This can be done using torch.nn loss functions, or by writing these functions from scratch. (3) We compute the gradients of the loss function with respect to model parameters. This can be done via the backward() method defined in the torch.autograd package. (4) We update the parameters via optimizer.step() using the torch.optim package where many commonly used optimizers have been defined. (5) Once the parameters have been updated, we normally set the parameter gradients to zero using the optimizer.zero_grad(), otherwise the gradient values keep accumulating from previous steps.

### [C] Evaluating models
For evaluating our models, we first set our model in eval mode via model.eval() and then make predictions inside the torch.inference_model() context manager. This disables computation of the gradients, which are only necessary for training. Another way to do this is via the torch.no_grad() context manager. Here and here are discussions on the pros and cons of using these different ways of preventing gradient computations.

### [D] Saving and loading models
PyTorch has a nice tutorial on saving and loading models. In brief, torch.save() saves a PyTorch object in Python's pickle format. torch.load() loads a saved PyTorch object. torch.nn.Module.load_state_dict() loads a model's saved state dictionary. PyTorch recommends that only state_dicts of the model (and the optimizer) are saved, that is, save via 
torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict()}, path) and load via model = ModelClass(); model.load_state_dict(torch.load(path))['model']. Instead of saving and loading the entire model, that is, save via torch.save(model, path) and load via model = torch.load(path) (# Model class must be defined somewhere).

<br>

## Computer Vision
Torchvision is PyTorch's library for CV. Torchvision.datasets provides common CV datasets and data loading functions for CV. Torchvision.transforms provides functions for manipulating images. Torch.utils.data provides functionality for creating custom datasets and dataloaders. 

<br>

## Custom Datasets
Torchvision.datasets also provides support for using custom datasets. First, let's talk about some useful Python functions for data preparation. The requests module (explained here) let's us send https requests over the internet. The zipfile module (explained here) allows us to work with zipped datasets. The pathlib module (explained here) provides a Path class. os.walk() helps us walk through a directory. Once we have folders of unzipped images, we need to turn them into tensors using one of the conversion functions torchvision.transforms.v2.to<>(), and then turn those into a torch.utils.data.Dataset and subsequently a torch.utils.data.DataLoader.

<br>

## Misc
### [A] Reproducibility
We can set the random seed for PyTorch using the torch.manual_seed() function. Read more about reproducibility here.

### [B] Running computations on GPUs
We want to do this because GPUs = faster computations on numbers thanks to CUDA, NVIDIA hardware and PyTorch doing work behind the scenes to leverage these. We can use a free GPU on Google Colab, or purchase our own GPU or rent one from AWS, Azure, etc. For the latter two options, we need to setup PyTorch and CUDA as discussed here. In code, we can check if GPU is available using torch.cuda.is_available() function. If available, we can set device = "cuda". Else we can set device = "cpu". We can get the number of available GPUs using the torch.cuda.device_count() function. We can copy a tensor to a device using the torch.Tensor.to() function, or directly create a tensor on a device by passing it in the torch.tensor() function. We can copy a tensor from a GPU back to a CPU using the torch.Tensor.cpu() function.

<br>

### Further Reading
 - Speeding up deep learning

### References
 - PyTorch documentation
 - PyTorch cheat sheet
 - PyTorch discussion forum
 - PyTorch in a day video tutorial





